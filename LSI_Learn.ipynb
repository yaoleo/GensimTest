{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LSI 概念\n",
    "\n",
    "LSI(Latent Semantic Indexing)，中文意译是潜在语义索引，即通过海量文献找出词汇之间的关系。\n",
    "\n",
    "基本理念是当两个词或一组词大量出现在一个文档中时，这些词之间就是语义相关的。\n",
    "\n",
    "有的文章也叫Latent Semantic  Analysis（LSA）。其实是一个东西，后面我们统称LSI，它是一种简单实用的主题模型。\n",
    "\n",
    "LSI是基于奇异值分解（SVD-Singular Value Decomposition）的方法来得到文本的主题的。\n",
    "\n",
    "LSA模型不是概率生成模型，因此，很难利用成熟的贝叶斯理论对其进行解释。\n",
    "\n",
    "## 什么是SVD？\n",
    "\n",
    "参考：http://www.cnblogs.com/pinard/p/6251584.html\n",
    "\n",
    "## LSI 理解\n",
    "对于一个m×n的矩阵A，可以分解为下面三个矩阵：\n",
    "\n",
    "\tAm×n=Um×mΣm×nVn×nT\n",
    "有时为了降低矩阵的维度到k，SVD的分解可以近似的写为：\n",
    "\n",
    "\tAm×n≈Um×kΣk×kVk×nT\n",
    "\n",
    "输入的有m个文本，每个文本有n个词。\n",
    "\n",
    "而Aij则对应第i个文本的第j个词的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。\n",
    "k是我们假设的主题数，一般要比文本数少。\n",
    "\n",
    "SVD分解后，\n",
    "Uil对应第i个文本和第l个主题的相关度。\n",
    "Σlm对应第l个主题和第m个词义的相关度。\n",
    "Vmj对应第m个词义和第j个词的相关度。\n",
    "\n",
    "反过来解释：\n",
    "\n",
    "输入的有m个词，对应n个文本。\n",
    "而Aij则对应第i个词档的第j个文本的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。\n",
    "k是我们假设的主题数，一般要比文本数少。\n",
    "SVD分解后，\n",
    "Uil对应第i个词和第l个词义的相关度。\n",
    "Σlm对应第l个词义和第m个主题的相关度。\n",
    "Vjm对应第j个文本和第m个主题的相关度。\n",
    "\n",
    "\n",
    "这样我们通过一次SVD，就可以得到文档和主题的相关度，词和词义的相关度以及词义和主题的相关度。\n",
    "\n",
    "## LSI计算文本相似度\n",
    "\n",
    "通过余弦相似度：\n",
    "\n",
    "参考：http://www.ruanyifeng.com/blog/2013/03/cosine_similarity.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## LSI主题模型总结\n",
    "\n",
    "LSI是最早出现的主题模型了，它的算法原理很简单，一次奇异值分解就可以得到主题模型，同时解决词义的问题，非常漂亮。\n",
    "\n",
    "但是LSI有很多不足，导致它在当前实际的主题模型中已基本不再使用。\n",
    "主要的问题有：\n",
    "+ SVD计算非常的耗时，尤其是我们的文本处理，词和文本数都是非常大的，对于这样的高维度矩阵做奇异值分解是非常难的。\n",
    "+ 主题值的选取对结果的影响非常大，很难选择合适的k值。\n",
    "+ LSI得到的不是一个概率模型，缺乏统计基础，结果难以直观的解释。\n",
    "\n",
    "对于问题1），主题模型非负矩阵分解（NMF）可以解决矩阵分解的速度问题。\n",
    "对于问题2），这是老大难了，大部分主题模型的主题的个数选取一般都是凭经验的，较新的层次狄利克雷过程（HDP）可以自动选择主题个数。\n",
    "对于问题3），牛人们整出了pLSI(也叫pLSA)和隐含狄利克雷分布(LDA)这类基于概率分布的主题模型来替代基于矩阵分解的主题模型。\n",
    "\n",
    "回到LSI本身，对于一些规模较小的问题，如果想快速粗粒度的找出一些主题分布的关系，则LSI是比较好的一个选择，其他时候，如果你需要使用主题模型，推荐使用LDA和HDP。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n",
    "\n",
    "隐含狄利克雷分布(Latent Dirichlet Allocation，以下简称LDA)。\n",
    "注意机器学习还有一个LDA，即线性判别分析，主要是用于降维和分类的\n",
    "\n",
    "## 朴素贝叶斯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
